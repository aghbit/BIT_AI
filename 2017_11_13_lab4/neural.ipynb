{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi!\n",
    "\n",
    "Today we're going to finish the topic of logitic regression, and then talk about the mightiest buzzword of them all - **neural networks**. The topic is very complex and you should treat today only as an introduction you can and should follow up on. We're merely scratching the surface!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But first - Logistic regression (again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick rewind\n",
    "$$h_w(x) = \\sigma(\\sum_{j=0}^k x_j w_j ) = \\sigma(xw)$$\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x = np.linspace(-10, 10)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we introduced te topic of classification - essentially training our models to decide whether an example 'is' or 'isn't' something. But in real-life, such a binary use case is sometimes *just* not enough.\n",
    "\n",
    "https://www.youtube.com/watch?v=ACmydtFDTGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "To solve the problem of classifying an object as one of multiple classes, we do a one-vs-all prediction. \n",
    "Previously we calculated $h_w(x)$ and applied $sigmoid$ function to it, to calculate the 'probablility' of our example being positive or not. Since $\\hat{y} \\in [0,1]$ We chose a 'threshold' in that range below which we can treat our example as negative and above which - as positive.\n",
    "\n",
    "For multiple classes, we must essentially calculate a hypothesis for **every single one** of possible categories. If hypothesis for a given category is high enough, there is a high probability that our object is of that category. In the other case, it means that it belongs to some other category (but we don't know which one - we need other hypotheses for that). \n",
    "\n",
    "![alt text](img/multiclass.PNG)\n",
    "This is called one-versus-all classification. Ultimately we choose the category whose hypothesis has the highest probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before\n",
    "\n",
    "Until now, a hypothesis $h_w(x^{(i)})$ for a given object $x^{(i)}$ represented as a vector of features $[x_0^{(i)}, x_1^{(i)}, ... x_k^{(i)}]$ was represented by a scalar:\n",
    "\n",
    "$$h_w(x) = \\sigma(\\sum_{j=0}^k w_j x_j) = \\sigma(wx)$$\n",
    "\n",
    "Where w was a vector a weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now\n",
    "\n",
    "If $m$ is the number of possible categories, then for every vector of features we want to perform multiple logistic regressions (for every possible category we might classify it as). \n",
    "Essentially, for every vector of $k$ features we now want to obtain a vector of $m$ hypothesis scalars:\n",
    "\n",
    "$$[x_0^{(i)}, x_1^{(i)}, ... x_k^{(i)}] \\xrightarrow{\\text{classification}} [h_0^{(i)}, h_1^{(i)}, ... h_m^{(i)}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every logistic regression we need a separate $k$-dimensional vector (or a $k \\times 1$ matrix) of weights. \n",
    "If we want to vectorize our computations, we can merge all of the weights vectors into a single, $k \\times m$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To sum it up\n",
    "- $n$ - number of examples in the dataset (objects we want to classify)\n",
    "- $k$ - number of features every object has\n",
    "- $m$ - number of possible categories to classify as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ - an $n \\times k$ matrix representing the examples\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_0^{(1)} & x_1^{(1)}  &  & ...  &x_k^{(1)}\\\\ \n",
    "x_0^{(2)} &...  &  &...  & \\\\ \n",
    "... &  &  &...  & \\\\ \n",
    "x_0^{(n)} &  & ... &  & x_k^{(n)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ - an $k \\times m$ matrix representing weights in logistic regression for every feature in every category\n",
    "\n",
    "\\begin{bmatrix}\n",
    "w_0^{(1)} & w_0^{(2)}  &  & ...  &w_0^{(m)}\\\\ \n",
    "w_1^{(1)} &...  &  &...  & \\\\ \n",
    "... &  &  &...  & \\\\ \n",
    "w_k^{(1)} &  & ... &  & w_k^{(m)}\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_W(X)$ - an $n \\times m$ matrix representing hypothesis vectors for every example and category \n",
    "\n",
    "$$\n",
    "h_W(X) = \\sigma(XW)\n",
    "$$\n",
    "\n",
    "We'll denote j-th hypothesis of i-th example as $$h_w^{(j)}(x^{(i)})$$\n",
    "Computationally-wise, the only thing that changes is the $m$ dimension of W.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for cost function...\n",
    "\n",
    "$$ L^{(j)}(w) = -\\sum_{i=0}^n y^{(i,j)}\\log{h_w^{(j)}(x^{(i)})} + (1-y^{(i,j)})\\log{(1-h_w(x^{(i)}))}$$\n",
    "\n",
    "We have a vector of cost values for every category $j$, which is useful in updating weights in gradient descent. If we want to plot the cost function, we can sum or count the mean of all those values.\n",
    "\n",
    "Gradient descent also works the same way as before.\n",
    "\n",
    "#### WTF is $y^{(i,j)}$?\n",
    "\n",
    "We can now look at y as a matrix of one-hot values. If $y^{(i,j)} = 1 $, then example $i$ is of class $j$. \n",
    "\n",
    "This also means the rest of values in $y^{(i)}$ are, of course, zeros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint - if you implemented those functions correctly last time, you can just paste your implementations below\n",
    "\n",
    "def hypotheses(W, X):\n",
    "    # implement me!\n",
    "\n",
    "\n",
    "def cost(W, X, Y, eps=0.01):\n",
    "    # implement me!\n",
    "\n",
    "\n",
    "def gradient_step(W, X, Y,learning_rate=0.01):\n",
    "    # implement me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = solutions._hypotheses\n",
    "cost = solutions.cost\n",
    "gradient_step = solutions.gradient_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 0.5, 1], [1, 1, 0, 0], [1, 0, 1, 0]])\n",
    "Y = np.array([[0, 1], [1, 0], [0, 1]])\n",
    "W = np.random.random((4,2))\n",
    "costs = []\n",
    "steps = 10000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, X, Y)\n",
    "    costs.append(cost(W, X, Y))\n",
    "\n",
    "plt.plot(np.arange(steps), costs)\n",
    "plt.show()\n",
    "print(hypotheses(W, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST - something more ambitious\n",
    "\n",
    "MNIST is one of the most famous datasets for beginers in Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist_dir = '/tmp/mnist'\n",
    "mnist = fetch_mldata('MNIST original', data_home=mnist_dir)\n",
    "print(mnist.data.shape)\n",
    "img = mnist.data[0]\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image of a number can be visualized as array of $784 (= 28*28)$ numbers, or jus a picture. \n",
    "For convenience values of pixels are stored not as a 2D array, but as a vector, so in order to be displayed, the vector must be reshaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = img.reshape(28,28) / 255\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat every single pixel as a separate feature. In order to do so, let's normalize them. We'll also create one-hot vectors of labels we can fit our model to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_count = mnist.data.shape[0]\n",
    "labels = mnist.target.astype(int)\n",
    "normalized_pixels_nobias = mnist.data / 255\n",
    "one_hot_labels = np.zeros((examples_count, 10))\n",
    "one_hot_labels[np.arange(examples_count), labels] = 1\n",
    "print(one_hot_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mnist_elem(index):\n",
    "    img = mnist.data[rand_no]\n",
    "    pixels = img.reshape(28,28) / 255\n",
    "    plt.imshow(pixels, cmap='gray')\n",
    "    plt.show()\n",
    "    print('label:', labels[rand_no])\n",
    "    print('label as a one-hot vector:', one_hot_labels[rand_no])\n",
    "\n",
    "examples_count = normalized_pixels_nobias.shape[0]\n",
    "rand_no = np.random.randint(0, examples_count)\n",
    "display_mnist_elem(rand_no)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides normalizing, a bias feature must be added to all examples making the count of features 785.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pixels = solutions.add_bias_feature(normalized_pixels_nobias) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's randomly pick training data from our dataset. In this example we're going to pick only 10000 examples for the sake of training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_numbers = np.arange(examples_count)\n",
    "np.random.shuffle(rand_numbers)\n",
    "\n",
    "train_count = 10000\n",
    "train_numbers = rand_numbers[:train_count]\n",
    "X_train = np.array([normalized_pixels[i] for i in range(examples_count) if i in train_numbers])\n",
    "Y_train = np.array([one_hot_labels[i] for i in range(examples_count) if i in train_numbers])\n",
    "\n",
    "X_test = np.array([normalized_pixels[i] for i in range(examples_count) if i not in train_numbers])\n",
    "\n",
    "Y_test = np.array([mnist.target[i] for i in range(examples_count) if i not in train_numbers])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we train our model on a limited number of examples ( $10000 = \\frac{1}{7}$ of the dataset) and for a relatively short number of epochs ($1000$). You are more than welcome to re-run this code with different numbers to see how they will affect the accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.random((785,10)) # 784 + bias feature\n",
    "costs = []\n",
    "steps = 1000\n",
    "\n",
    "for i in range(steps):\n",
    "    W = gradient_step(W, X_train, Y_train)\n",
    "    costs.append(cost(W, X_train, Y_train))\n",
    "\n",
    "plt.plot(np.arange(steps), costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_no = np.random.randint(0, examples_count)\n",
    "display_mnist_elem(rand_no)\n",
    "img_pixels = normalized_pixels[rand_no]\n",
    "predicted_H = hypotheses(W, img_pixels)\n",
    "predicted_class = np.argmax(predicted_H)\n",
    "\n",
    "print('predicted hypotheses:', predicted_H)\n",
    "print('predicted_class:', predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's test the accuracy on the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_test = hypotheses(W, X_test)\n",
    "predicted_test_labels = np.argmax(H_test, axis=1)\n",
    "accurate = predicted_test_labels == Y_test\n",
    "len([a for a in accurate if a]) / len(accurate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For MNIST it's actually embarassingly bad (best models achieve even 99.9% accuracy), but it's not so bad for one matrix trained on $\\frac{1}{7}$ of the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We promised you neural networks today, though!\n",
    "\n",
    "Now that you understand how logistic regression works, you'll see that at a first glance, neural networks have some similarities to them.\n",
    "\n",
    "## Layers\n",
    "\n",
    "Neural networks are made up of layers. Every layer transorms the data given to it by previous layer in the form of **activation values** and conveys the output to the next one.\n",
    "\n",
    "![alt text](img/neural_schema.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some old/new notations\n",
    "\n",
    "#### activation - values passed as an input to a layer\n",
    "$a^{(j)}$ - activation values in layer $j$ (a vector)\n",
    "\n",
    "#### layer matrices\n",
    "$W^{(j)}$ - matrix of weights mapping activation of layer $j$ into layer $j+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "For a single feature vector (single example) $x$:\n",
    "\n",
    "Activation of the first (input) layer is simply the (normalized) feature vector (including the bias feature!) of our example:\n",
    "\n",
    "$$a^{(1)} = x$$\n",
    "\n",
    "We denote output of the $(j-1)^{th}$ layer as:\n",
    "\n",
    "$$z^{(j)} = a^{(j-1)}W^{(j-1)}$$ (vector $\\times$ matrix, so outputs are vectors).\n",
    "\n",
    "Activation of $j^{th}$ layer can then be calculated as:\n",
    "\n",
    "$$a^{(j)} = g(z^{(j)})$$\n",
    "\n",
    "Where $g$ is an **activation function** - for example sigmoid.\n",
    "\n",
    "**After** calculating $a^{(j)}$ vector, bias feature must be added to it.\n",
    "\n",
    "The final output - our hypothesis - is the output of the final layer with activation function applied to it. So if the network has $L$ layers, the equation is:\n",
    "$$h_W(x) = a^{(L+1)} = g(z^{(L+1)})$$\n",
    "\n",
    "Though this example was described with only one feature vector $x$, it's analogous when there are more examples and vector becomes a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the similarities with logistic regression?\n",
    "\n",
    "In a way, you could think of it as network learning it's own features. Through adding more layers, we are able to calculate and represent more complex, non-linear models more elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w_1, w_2, w_3):\n",
    "    # assuming X is normalized and contains bias features\n",
    "    # you could of course extend the function to more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_prop = solutions.forward_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But wait! What about cost? Optimization? Learning?\n",
    "\n",
    "Learning happens through **backpropagation** algorithm which is all about propagation of errors through the network and is a bitch to implement in such a short time. You can read more about it here:\n",
    "\n",
    "http://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is one of the main deep learning frameworks/libraries in deep learning (what is deep learning?!). There are many others libraries in Python, they differ in level of abstraction (high-level/low-level) and in building computational graph of neural network (static/dynamic, more or less like difference between Java & Python).\n",
    "\n",
    "Today we will talk about one of them which is Keras. It takes high-level static approach. There is backend operating underneath where all computations are done. The main used backend nowadays is TensorFlow and if you don't have reason to switch, stick to TF, which is enabled by default.\n",
    "\n",
    "https://keras.io/backend/\n",
    "\n",
    "Keras provides really nice API to build and train neural network, it reads like pseudocode while it is real executable code. It also behaves in scikit-learn-like manner (fit/predict interface).\n",
    "\n",
    "Let the fun begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your keras works correctly there should be something like \"Using TensorFlow backend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've initialized a basic model that will be described in Sequential way. There is also another way of describing it, which allows having cycles in our model etc. (for recurrent neural networks).\n",
    "\n",
    "Let's add a layer! (Let's Dense!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation='sigmoid', input_shape=[784]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dense\" stands for the the typical layer which connects every previous node with every next node* as in the picture. It is also called \"fully-connected\".\n",
    "\n",
    "*(for example convolutional connections are not dense, we will not talk about them today)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well a neural network will do on MNIST. We'll start by splitting the data into training/test sets again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pixels = normalized_pixels_nobias\n",
    "\n",
    "# normalization is important\n",
    "# normalized_pixels = normalized_pixels * 255\n",
    "\n",
    "rand_numbers = np.arange(examples_count)\n",
    "np.random.shuffle(rand_numbers)\n",
    "\n",
    "train_count = 10000\n",
    "train_numbers = rand_numbers[:train_count]\n",
    "X_train = np.array([normalized_pixels[i] for i in range(examples_count) if i in train_numbers])\n",
    "Y_train = np.array([one_hot_labels[i] for i in range(examples_count) if i in train_numbers])\n",
    "\n",
    "X_test = np.array([normalized_pixels[i] for i in range(examples_count) if i not in train_numbers])\n",
    "\n",
    "Y_test = np.array([mnist.target[i] for i in range(examples_count) if i not in train_numbers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train) # biases are added automatically under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should get \"RuntimeError: The model needs to be compiled before being used.\". That's one of the signs that we take static approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # we can put different loss here\n",
    "              optimizer=keras.optimizers.sgd(), # we can have different optimizers\n",
    "              metrics=['accuracy']) # we can measure our performance with many metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD stands for Stochastic Gradient Descent which is a modification of standard Gradient Descent. Here in each iteration you sample randomly a batch (let's say 64 samples) from your dataset and do update only on it. It is better because sometimes one sample is so big that you can't put your entire dataset in memory for computing (especially while using GPU, which allows us to train nets muuuuuuuuuuch faster).\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a bit about what is batch (baby don't train me, don't train me, no more). https://youtu.be/3rzgrP7VA_Q\n",
    "\n",
    "And a bit what is epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test) #loss and accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say it straight, your model is not good, it achieves only ~21% of accuracy. Maybe we can do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation='softmax', input_shape=(784,))\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', # we can put different loss here\n",
    "              optimizer=keras.optimizers.SGD(), # we can have different optimizers\n",
    "              metrics=['accuracy']) # we can measure our performance with many metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used softmax activation (which is used in almost every output layer in neural networks nowadays).\n",
    "But our NN has only one layer which is output layer. Let's make a one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation='relu', input_shape=(784,)), # notice RELU!!!\n",
    "\n",
    "    keras.layers.Dense(10, activation='softmax') # input shape inferred automatically, yaay!\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.sgd(),\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you noticed the relu activation! Let's stop for a bit an show how that function looks on blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use better optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(100, activation='relu', input_shape=(784,)),\n",
    "\n",
    "    keras.layers.Dense(10, activation='softmax') # input shape inferred automatically, yaay!\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.Adam(), # daÄ‡ nowy optimajzer? A dam! (hehe)\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woooooow! You've probably achieved score higher than 80%. At that's \"only\" with change of optimizer. \"Adam\" is the most widely used type of optimizer. They are all well described here, read it after our workshop! All used optimizers are variation of simple GD.\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up. We can build a neural network like a lego blocks, set up parameters of every block and easily train it. We can change how we measure our loss, how we train our net, how we measure performance. We can set size of batch and size of epoch.\n",
    "\n",
    "There are many many different layers (convolutional, recurrent etc.) and each of them is used for different purpose.\n",
    "\n",
    "The official keras site is really great source of examples and documentation of all the parts described today.\n",
    "\n",
    "https://keras.io/ \n",
    "\n",
    "Let's look at it!\n",
    "\n",
    "Read the official tutorial on MNIST with Multi Layer Perceptron, which is just another name for normal feedforward (as opposed to recurrent) fully-connected (dense) neural network with many layers. \n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "There are many really good examples in this repository, please read them and another parts of the code (how is everything implemented etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now just experiment and build the network that achieves the best accuracy on MNIST!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
